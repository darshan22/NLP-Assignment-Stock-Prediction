{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import calendar\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Keras imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Bidirectional, Embedding, LSTM, Dense, Conv1D, GlobalMaxPool1D, MaxPool1D, MaxPooling1D, Dropout, Activation , Flatten , Input, concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reutersFile = 'news_reuters.csv'\n",
    "stockFile = 'stockReturns.json'\n",
    "df1 = pd.read_csv('news_reuters.csv', header=None, \n",
    "                  names=['ticker', 'company', 'pub_date', 'headline', 'first_sent', 'category'])\n",
    "df2 = pd.read_json('stockReturns.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Reformat, clean, merge, and tokenize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **reformat_y_data**\n",
    "    - Convert stock data from continuous numeric data normalized to S&P returns into binary (postive/negative)\n",
    "\n",
    "2. **clean_and_merge_data**\n",
    "    - Filter X to only those tickers with stock data\n",
    "3. **clean_text**\n",
    "    - Replacing double spaces into a single space\n",
    "    - Replace U.S. to United States so U won't get deleted with next replacement\n",
    "    - Remove all capitalized words at the beginning of the sentence, since those are mostly places (aka NEW YORK)\n",
    "    - Remove unnecessary punctuation (hyphens and asterisks)\n",
    "    - Remove dates\n",
    "4. **tokenize_sent**\n",
    "    - Tokenize string into a sequence of words\n",
    "5. **filt_to_one**\n",
    "    - Filter dataset so that there is only one observation per day.\n",
    "    - If there is more than one record, will use the topStory record if one exists.  \n",
    "    - If one doesn't or there are 2 topStory records then it will randomly select one of the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_y_data(data, tickerType='mid'):\n",
    "    tmp = data[tickerType].apply(pd.Series)\n",
    "    tmp = tmp.stack().rename('price', inplace=True).reset_index()\n",
    "    tmp['y'] = np.where(tmp['price'] >= 0, 1, 0)\n",
    "    tmp.rename(columns={'level_0': 'ticker', 'level_1': 'pub_date'}, inplace=True)\n",
    "    return tmp\n",
    "\n",
    "def clean_and_merge_data(X, Y):\n",
    "    y_tickers = set(Y['ticker'])\n",
    "    X = X.loc[X['ticker'].isin(y_tickers)]\n",
    "    # Make sure data types are the same for merge    \n",
    "    Y['pub_date'] = Y['pub_date'].astype(df1['pub_date'].dtype)\n",
    "    Y['ticker'] = Y['ticker'].astype(df1['ticker'].dtype)\n",
    "    return X.merge(Y, on=['ticker', 'pub_date'], how='left')\n",
    "\n",
    "def clean_text(sent):\n",
    "    monthStrings = list(calendar.month_name)[1:] + list(calendar.month_abbr)[1:]\n",
    "    monthPattern = '|'.join(monthStrings)\n",
    "    sent = re.sub(r' +', ' ', sent)\n",
    "    sent = re.sub(r'U.S.', 'United States', sent)\n",
    "    sent = re.sub(r'^(\\W?[A-Z\\s\\d]+\\b-?)', '', sent)\n",
    "    sent = re.sub(r'^ ?\\W ', '', sent)\n",
    "    sent = re.sub(r'({}) \\d+'.format(monthPattern), '', sent)\n",
    "    sent = re.sub(r' +', ' ', sent)\n",
    "    return sent \n",
    "\n",
    "def tokenize_sent(col):\n",
    "    return [text_to_word_sequence(text, lower=False) for text in col]\n",
    "\n",
    "def filt_to_one(x, random_state=10):\n",
    "    if x.shape[0] > 1:\n",
    "        if 'topStory' in x['category'].unique():\n",
    "            x = x.loc[x['category'] == 'topStory']\n",
    "        if x.shape[0] > 1:\n",
    "            x = x.sample(n=1, random_state=random_state)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanY = reformat_y_data(df2, 'short')\n",
    "merged = clean_and_merge_data(df1, cleanY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Clean up the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up text\n",
    "merged['headline'] = merged.headline.apply(clean_text)\n",
    "merged['first_sent'] = merged.first_sent.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn sentences into tokens\n",
    "merged['headline_token'] = tokenize_sent(merged.headline)\n",
    "merged['first_sent_token'] = tokenize_sent(merged.first_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one record per company/day\n",
    "finalData = merged.groupby(by=['ticker', 'pub_date']).apply(filt_to_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Headline and First Sentence into one text \n",
    "finalData['final_text'] = finalData['headline_token'] + finalData.first_sent_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove observations with missing stock price\n",
    "finalData.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = ['ticker2', 'company', 'pub_date2', \n",
    "            'headline', 'first_sent', 'category', \n",
    "            'price', 'y', 'headline_token', \n",
    "            'first_sent_token', 'final_text']\n",
    "finalData.columns = new_columns\n",
    "finalData.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = finalData['headline'].values\n",
    "y = finalData['y'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Create a train and test set, retaining the same test set for every model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into training and testing sets and stratify on y\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_NUM_WORDS=40 \n",
    "\n",
    "#max number of words in a review to use\n",
    "MAX_SEQUENCE_LENGTH=100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit tokenizer on X_train\n",
    "tokenizer = Tokenizer(num_words=200)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create padded sequences\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "train_data = pad_sequences(sequences, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert y_train to one-hot encoded version\n",
    "word_index = tokenizer.word_index\n",
    "y_train_labels = to_categorical(np.asarray(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (8542, 100)\n",
      "Shape of label tensor: (8542, 2)\n"
     ]
    }
   ],
   "source": [
    "#check shape of train_data and y_train_labels\n",
    "print('Shape of data tensor:', train_data.shape)\n",
    "print('Shape of label tensor:', y_train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit tokenizer on X_test\n",
    "tokenizer_test = Tokenizer(num_words=200)\n",
    "tokenizer_test.fit_on_texts(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create padded sequences\n",
    "sequences_test = tokenizer_test.texts_to_sequences(X_test)\n",
    "test_data = pad_sequences(sequences_test, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert y_test to one-hot encoded version\n",
    "word_index_text = tokenizer_test.word_index\n",
    "y_test_labels = to_categorical(np.asarray(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (2848, 100)\n",
      "Shape of label tensor: (2848, 2)\n"
     ]
    }
   ],
   "source": [
    "#check shape of test_data and y_test_labels\n",
    "print('Shape of data tensor:', test_data.shape)\n",
    "print('Shape of label tensor:', y_test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Load word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I assume that you have the 'glove.6B.100d.txt' file in your directory\n",
    "GLOVE_DIR=''\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Create Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the size of each word vector\n",
    "EMBEDDING_DIM = 100 \n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "       # words not found in embedding index will be all-zeros.\n",
    "       embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_model(seq_input_len, embed_matrix, \n",
    "                     n_RNN_nodes, n_dense_nodes, \n",
    "                     recurrent_dropout=0.2, \n",
    "                     drop_out=.2, n_out=2):\n",
    "    \n",
    "    word_input = Input(shape=(seq_input_len,), name='word_input_layer')\n",
    "    word_embeddings = Embedding(input_dim=embed_matrix.shape[0],\n",
    "                                output_dim=embed_matrix.shape[1],\n",
    "                                weights=[embed_matrix], \n",
    "                                mask_zero=True, \n",
    "                                name='word_embedding_layer')(word_input) \n",
    "    hidden_layer1 = Bidirectional(LSTM(units=n_RNN_nodes, return_sequences=True, \n",
    "                                      recurrent_dropout=recurrent_dropout, \n",
    "                                      dropout=drop_out, name='hidden_layer1'))(word_embeddings)\n",
    "    hidden_layer2 = Bidirectional(LSTM(units=n_RNN_nodes, return_sequences=False, \n",
    "                                      recurrent_dropout=recurrent_dropout,\n",
    "                                      dropout=drop_out, name='hidden_layer2'))(hidden_layer1)\n",
    "    dense_layer = Dense(units=n_dense_nodes, activation='relu', name='dense_layer')(hidden_layer2)\n",
    "    drop_out3 = Dropout(drop_out)(dense_layer)\n",
    "    output_layer = Dense(units=n_out, activation='softmax',\n",
    "                         name='output_layer')(drop_out3)\n",
    "    model = Model(inputs=[word_input], outputs=output_layer)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", \n",
    "                  metrics=['accuracy', recall, precision])\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Define functions to calculate precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out = 2\n",
    "nb_epoch = 1\n",
    "rnn_model = create_rnn_model(seq_input_len=train_data.shape[-1],\n",
    "                             embed_matrix=embedding_matrix, \n",
    "                             recurrent_dropout=.4, drop_out=.5,\n",
    "                             n_RNN_nodes=500, n_dense_nodes=500, n_out=n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(model, x_train, y_train, x_test, y_test, \n",
    "                         modelSaveName, modelSavePath='',\n",
    "                         batch_size=1014, epochs=2, validation_split=.1):\n",
    "    print(model.summary())\n",
    "    \n",
    "    filepath = os.path.join(modelSavePath, modelSaveName + '.hdf5')\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1)\n",
    "    callbacks_list = [checkpoint]\n",
    "    model.fit(x=x_train, y=y_train, batch_size=batch_size, \n",
    "              epochs=epochs, validation_split=validation_split, \n",
    "              callbacks=callbacks_list)\n",
    "    \n",
    "    score, acc, rec, prec = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    return (model, acc, rec, prec)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "word_input_layer (InputLayer (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "word_embedding_layer (Embedd (None, 100, 100)          1022000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 1000)         2404000   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 1000)              6004000   \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 2)                 1002      \n",
      "=================================================================\n",
      "Total params: 9,931,502\n",
      "Trainable params: 9,931,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7687 samples, validate on 855 samples\n",
      "Epoch 1/1\n",
      "7687/7687 [==============================] - 697s 91ms/step - loss: 0.7133 - acc: 0.4936 - recall: 0.4936 - precision: 0.4936 - val_loss: 0.6925 - val_acc: 0.5205 - val_recall: 0.5205 - val_precision: 0.5205\n",
      "\n",
      "Epoch 00001: saving model to rnn_model.hdf5\n",
      "2848/2848 [==============================] - 72s 25ms/step\n"
     ]
    }
   ],
   "source": [
    "rnn_res = train_and_test_model(rnn_model, train_data, \n",
    "                               y_train_labels, test_data, \n",
    "                               y_test_labels, 'rnn_model',\n",
    "                               epochs=nb_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentences(data, lexicon, maxlen=200):\n",
    "    X = []\n",
    "    for sentences in data:\n",
    "        x = [lexicon[token] if token in lexicon else lexicon['<UNK>'] for \n",
    "                                 token in sentences]\n",
    "        x2 = np.eye(len(char_indices) + 1)[x]\n",
    "        X.append(x2)\n",
    "    return (pad_sequences(X, maxlen=maxlen))\n",
    "\n",
    "def create_cnn_model(char_maxlen, vocab_size,\n",
    "                     nb_filter=100, filter_kernels = [4] * 4,\n",
    "                     pool_size=3, n_dense_nodes=100,\n",
    "                     drop_out=.2, n_out=2):\n",
    "\n",
    "    inputs = Input(shape=(char_maxlen, vocab_size), name='char_input_layer')\n",
    "\n",
    "    conv1 = Conv1D(nb_filter, kernel_size=filter_kernels[0],\n",
    "                  padding='valid', activation='relu',\n",
    "                  input_shape=(char_maxlen, vocab_size))(inputs)\n",
    "    \n",
    "    maxpool1 = MaxPool1D(pool_size=pool_size)(conv1)\n",
    "\n",
    "    conv2 = Conv1D(nb_filter, kernel_size=filter_kernels[1],\n",
    "                          padding='valid', activation='relu')(maxpool1)\n",
    "    maxpool2 = MaxPool1D(pool_size=pool_size)(conv2)\n",
    "\n",
    "    conv3 = Conv1D(nb_filter, kernel_size=filter_kernels[2],\n",
    "                          padding='valid', activation='relu')(maxpool2)\n",
    "\n",
    "    conv4 = Conv1D(nb_filter, kernel_size=filter_kernels[3],\n",
    "                          padding='valid', activation='relu')(conv3)\n",
    "\n",
    "    maxpool3 = MaxPool1D(pool_size=pool_size)(conv4)\n",
    "    flatten = Flatten()(maxpool3)\n",
    "\n",
    "    dense_layer = Dense(n_dense_nodes, activation='relu')(flatten)\n",
    "    dropout = Dropout(drop_out)(dense_layer)\n",
    "\n",
    "    output_layer = Dense(n_out, activation='softmax', name='output')(dropout)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", \n",
    "                  metrics=['accuracy', recall, precision])    \n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_maxlen = 1024 \n",
    "nb_filter = 128\n",
    "dense_outputs = 1024\n",
    "filter_kernels = [7, 5, 5, 3]\n",
    "pool_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn all tokens into one string and then all obs \n",
    "# into one overall string\n",
    "trainTokensAsString = X_train\n",
    "testTokensAsString = X_test\n",
    "oneTxt = ' '.join(trainTokensAsString)\n",
    "\n",
    "# Get info about characters\n",
    "chars = set(oneTxt)\n",
    "vocab_size = len(chars) + 1\n",
    "print('total chars:', vocab_size)\n",
    "char_indices = dict((c, i + 2) for i, c in enumerate(chars))\n",
    "indices_char = dict((i + 2, c) for i, c in enumerate(chars))\n",
    "\n",
    "char_indices['<UNK>'] = 1\n",
    "indices_char[1] = '<UNK>'\n",
    "\n",
    "trainCharData = vectorize_sentences(trainTokensAsString, char_indices, char_maxlen)\n",
    "testCharData = vectorize_sentences(testTokensAsString, char_indices, char_maxlen)\n",
    "trainCharData.shape\n",
    "testCharData.shape\n",
    "char_maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_input_layer (InputLayer (None, 1024, 92)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 1018, 128)         82560     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 203, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 199, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 33, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 6, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              787456    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 1,085,442\n",
      "Trainable params: 1,085,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7687 samples, validate on 855 samples\n",
      "Epoch 1/1\n",
      "7687/7687 [==============================] - 67s 9ms/step - loss: 0.6933 - acc: 0.4916 - recall: 0.4290 - precision: 0.5220 - val_loss: 0.6932 - val_acc: 0.4795 - val_recall: 0.4795 - val_precision: 0.4795\n",
      "\n",
      "Epoch 00001: saving model to cnn_model.hdf5\n",
      "2848/2848 [==============================] - 12s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "cnn_model = create_cnn_model(char_maxlen=char_maxlen, \n",
    "                             vocab_size=vocab_size,\n",
    "                             nb_filter=nb_filter, \n",
    "                             filter_kernels=filter_kernels,\n",
    "                             pool_size=pool_size, \n",
    "                             n_dense_nodes=dense_outputs,\n",
    "                             drop_out=.5, \n",
    "                             n_out=n_out)\n",
    "\n",
    "cnn_res = train_and_test_model(cnn_model, trainCharData[:, :, 1:],\n",
    "                               y_train_labels, \n",
    "                               testCharData[:, :, 1:], \n",
    "                               y_test_labels, \n",
    "                               'cnn_model',\n",
    "                               epochs=nb_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: RNN+CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_rnn_model(rnn_input_len, char_maxlen, vocab_size,\n",
    "                         embed_matrix, n_RNN_nodes, \n",
    "                         nb_filter=100, filter_kernels = [4] * 4,\n",
    "                         pool_size=3, n_dense_nodes=100,\n",
    "                         recurrent_dropout=0.2, \n",
    "                         drop_out=.2, n_out=2):\n",
    "    \n",
    "    word_input = Input(shape=(rnn_input_len,), name='word_input_layer')\n",
    "    char_input = Input(shape=(char_maxlen, vocab_size), name='char_input_layer')\n",
    "    \n",
    "    word_embeddings = Embedding(input_dim=embed_matrix.shape[0],\n",
    "                                output_dim=embed_matrix.shape[1],\n",
    "                                weights=[embed_matrix], \n",
    "                                mask_zero=True, \n",
    "                                name='word_embedding_layer')(word_input) \n",
    "\n",
    "    rnn_output1 = Bidirectional(LSTM(units=n_RNN_nodes, return_sequences=True, \n",
    "                                      recurrent_dropout=recurrent_dropout, \n",
    "                                      dropout=drop_out, name='hidden_layer1'))(word_embeddings)\n",
    "    \n",
    "    rnn_output2 = Bidirectional(LSTM(units=n_RNN_nodes, return_sequences=False, \n",
    "                                      recurrent_dropout=recurrent_dropout,\n",
    "                                      dropout=drop_out, name='hidden_layer2'))(rnn_output1)\n",
    "            \n",
    "    conv1 = Conv1D(nb_filter, kernel_size=filter_kernels[0],\n",
    "                  padding='valid', activation='relu',\n",
    "                  input_shape=(char_maxlen, vocab_size))(char_input)\n",
    "\n",
    "    maxpool1 = MaxPool1D(pool_size=pool_size)(conv1)\n",
    "\n",
    "    conv2 = Conv1D(nb_filter, kernel_size=filter_kernels[1],\n",
    "                          padding='valid', activation='relu')(maxpool1)\n",
    "    maxpool2 = MaxPool1D(pool_size=pool_size)(conv2)\n",
    "\n",
    "    conv3 = Conv1D(nb_filter, kernel_size=filter_kernels[2],\n",
    "                          padding='valid', activation='relu')(maxpool2)\n",
    "\n",
    "    conv4 = Conv1D(nb_filter, kernel_size=filter_kernels[3],\n",
    "                          padding='valid', activation='relu')(conv3)\n",
    "\n",
    "    maxpool3 = MaxPool1D(pool_size=pool_size)(conv4)\n",
    "    cnn_output = Flatten()(maxpool3)\n",
    "\n",
    "    merged_layer = concatenate([cnn_output, rnn_output2])\n",
    "    \n",
    "    dense_layer1 = Dense(n_dense_nodes, activation='relu', name='dense_layer')(merged_layer)\n",
    "    drop_out1 = Dropout(drop_out)(dense_layer1)\n",
    "    dense_layer2 = Dense(n_dense_nodes, activation='relu')(drop_out1)\n",
    "    drop_out2 = Dropout(drop_out)(dense_layer2)\n",
    "    \n",
    "    main_output = Dense(n_out, activation='softmax', name='output_layer')(drop_out2)\n",
    "\n",
    "    model = Model(inputs=[word_input, char_input], outputs=[main_output])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", \n",
    "                  metrics=['accuracy', recall, precision])    \n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_rnn_model = create_cnn_rnn_model(rnn_input_len=train_data.shape[-1], \n",
    "                                     char_maxlen=char_maxlen, \n",
    "                                     vocab_size=vocab_size,\n",
    "                                     embed_matrix=embedding_matrix, \n",
    "                                     n_RNN_nodes=500,\n",
    "                                     nb_filter=nb_filter, \n",
    "                                     filter_kernels=filter_kernels,\n",
    "                                     pool_size=pool_size, \n",
    "                                     n_dense_nodes=400,\n",
    "                                     recurrent_dropout=0.4, \n",
    "                                     drop_out=.5, \n",
    "                                     n_out=n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input_layer (InputLayer)   (None, 1024, 92)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1018, 128)    82560       char_input_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 203, 128)     0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 199, 128)     82048       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 39, 128)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 35, 128)      82048       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "word_input_layer (InputLayer)   (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 33, 128)      49280       conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding_layer (Embedding (None, 100, 100)     1022000     word_input_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 6, 128)       0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 100, 1000)    2404000     word_embedding_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 768)          0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1000)         6004000     bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1768)         0           flatten_3[0][0]                  \n",
      "                                                                 bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer (Dense)             (None, 400)          707600      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 400)          0           dense_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 400)          160400      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 400)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 2)            802         dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 10,594,738\n",
      "Trainable params: 10,594,738\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 7687 samples, validate on 855 samples\n",
      "Epoch 1/1\n",
      "7687/7687 [==============================] - 775s 101ms/step - loss: 0.6985 - acc: 0.4838 - recall: 0.4828 - precision: 0.4834 - val_loss: 0.6978 - val_acc: 0.4830 - val_recall: 0.4830 - val_precision: 0.4830\n",
      "\n",
      "Epoch 00001: saving model to cnn_rnn_model.hdf5\n",
      "2848/2848 [==============================] - 81s 28ms/step\n"
     ]
    }
   ],
   "source": [
    "cnn_rnn_res = train_and_test_model(cnn_rnn_model, \n",
    "                               [train_data, trainCharData[:, :, 1:]],\n",
    "                               y_train_labels, \n",
    "                               [test_data, testCharData[:, :, 1:]],\n",
    "                               y_test_labels, \n",
    "                               'cnn_rnn_model',\n",
    "                               epochs=nb_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare performance of all of models in a table (precision and recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cnn_mod</th>\n",
       "      <td>0.500702</td>\n",
       "      <td>0.500702</td>\n",
       "      <td>0.500702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn_mod</th>\n",
       "      <td>0.501404</td>\n",
       "      <td>0.501404</td>\n",
       "      <td>0.501404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_rnn_mod</th>\n",
       "      <td>0.500351</td>\n",
       "      <td>0.500351</td>\n",
       "      <td>0.500351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             accuracy    recall  precision\n",
       "cnn_mod      0.500702  0.500702   0.500702\n",
       "rnn_mod      0.501404  0.501404   0.501404\n",
       "cnn_rnn_mod  0.500351  0.500351   0.500351"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_records([cnn_res[1:4], rnn_res[1:4], cnn_rnn_res[1:4]], \n",
    "                          columns=['accuracy', 'recall', 'precision'], \n",
    "                         index=['cnn_mod', 'rnn_mod', 'cnn_rnn_mod'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Look at your labeling and print out the underlying data compared to the labels - for each model print out 2-3 examples of a good classification and a bad classification. Make an assertion why your model does well or poorly on those outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classifications(classifications, classType, test_y, test_text):\n",
    "    texts = [''.join(sent) for sent in test_text[classifications]]\n",
    "    stock_movements = np.where(test_y[classifications], 'positive', 'negative')\n",
    "    print('Examples of {} predictions:\\n'.format(classType))\n",
    "    for i in range(len(texts)):\n",
    "        print('Stock movement was {}'.format(stock_movements[i]))\n",
    "        print('News info:\\n{}'.format(texts[i]))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_print_samples(model, modelName, test_x, test_y=y_test, test_text = X_test):\n",
    "    \"\"\"\"Print out predictions of the model\"\"\"\n",
    "    print('Stats for {} model'.format(modelName))\n",
    "    res = model.predict(test_x)\n",
    "    class_res = np.apply_along_axis(np.argmax, axis=1, arr=res)\n",
    "    comparisons = class_res == test_y\n",
    "    comparisons = pd.DataFrame(comparisons)\n",
    "    good_class = comparisons.loc[comparisons[0] == True].index[0:3]\n",
    "    bad_class = comparisons.loc[comparisons[0] == False].index[0:3]\n",
    "    print_classifications(good_class, 'correct', test_y, test_text)\n",
    "    print_classifications(bad_class, 'INcorrect', test_y, test_text)\n",
    "    y_test_df = pd.DataFrame(y_test)\n",
    "    top3MostProbPosArg = np.argsort(res[:, 1])[-3:]\n",
    "    top3Y = y_test_df.iloc[top3MostProbPosArg]\n",
    "    top3Probs = pd.Series(res[top3MostProbPosArg, 1], index=top3Y.index)\n",
    "    top3Data = pd.concat([top3Y, top3Probs], axis=1)\n",
    "    top3Data.columns = ['Actual', 'PositiveProb']\n",
    "    print('')\n",
    "    print('Top 3 Most Positive Probability:')\n",
    "    print(top3Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for RNN model\n",
      "Examples of correct predictions:\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Google refuses French order to apply 'right to be forgotten' globally \n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Disney axes Marvel's marketing department: report \n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Tech firms write to United States FCC to oppose 'net neutrality' plan \n",
      "\n",
      "Examples of INcorrect predictions:\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Futures advance with UMich data leading indicators due \n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Bristol-Myers receives positive CHMP opinion for Opdivo \n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Fondiaria-SAI says sold stake in Mediobanca \n",
      "\n",
      "\n",
      "Top 3 Most Positive Probability:\n",
      "      Actual  PositiveProb\n",
      "1907     1.0      0.532536\n",
      "1983     1.0      0.538668\n",
      "2394     0.0      0.544906\n",
      "Stats for CNN model\n",
      "Examples of correct predictions:\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Deals of the day-Mergers and acquisitions \n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "JRP says Barclays Deutsche Bank and RBS to arrange investor meets \n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Enanta Pharmaceuticals reports the appointment of Lesley Russell to its board of directors \n",
      "\n",
      "Examples of INcorrect predictions:\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Futures advance with UMich data leading indicators due \n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Bristol-Myers receives positive CHMP opinion for Opdivo \n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Google refuses French order to apply 'right to be forgotten' globally \n",
      "\n",
      "\n",
      "Top 3 Most Positive Probability:\n",
      "      Actual  PositiveProb\n",
      "2553     0.0      0.500405\n",
      "1770     1.0      0.500417\n",
      "384      1.0      0.500599\n",
      "Stats for CNN_RNN model\n",
      "Examples of correct predictions:\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Deals of the day-Mergers and acquisitions \n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "JRP says Barclays Deutsche Bank and RBS to arrange investor meets \n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Enanta Pharmaceuticals reports the appointment of Lesley Russell to its board of directors \n",
      "\n",
      "Examples of INcorrect predictions:\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Futures advance with UMich data leading indicators due \n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Bristol-Myers receives positive CHMP opinion for Opdivo \n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Google refuses French order to apply 'right to be forgotten' globally \n",
      "\n",
      "\n",
      "Top 3 Most Positive Probability:\n",
      "      Actual  PositiveProb\n",
      "661      0.0      0.574480\n",
      "2341     1.0      0.575507\n",
      "1219     0.0      0.577529\n"
     ]
    }
   ],
   "source": [
    "predict_and_print_samples(rnn_res[0], 'RNN', test_data)\n",
    "predict_and_print_samples(cnn_res[0], 'CNN', testCharData[:, :, 1:])\n",
    "predict_and_print_samples(cnn_rnn_res[0], 'CNN_RNN', [test_data, testCharData[:, :, 1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
